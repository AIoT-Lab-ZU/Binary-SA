{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23e822cc-c6a9-408f-bb2d-256e9b8dae76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version 2.6.0\n",
      "2.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MMM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import required packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dad3468-6a3a-4fd5-8282-ee5b18da5fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MMM\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.7.0 and strictly below 2.10.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.6.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fa5902c-94f5-482f-8cb3-9bbdcb9980b4",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mmgDSHjo-Zud"
   },
   "outputs": [],
   "source": [
    "# Import Sentiment140 dataset\n",
    "# Update file location of Sentiment140.csv according to your working environment\n",
    "\n",
    "data_path  = \"data/training.1600000.processed.noemoticon.csv\"\n",
    "\n",
    "twitter_data = pd.read_csv(data_path,names=['target','id','date','flag','user','text'],\n",
    "                           encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a0e5caf-ac80-4ba0-a6f1-d7c1e9e281ce",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4clzU4R-_IqE"
   },
   "outputs": [],
   "source": [
    "# Create NumPy array of unprocessed input text and target \n",
    "\n",
    "X=np.array(twitter_data['text'])\n",
    "Y=np.array(twitter_data['target'])\n",
    "\n",
    "# Set Y=1 for Positive Tweets\n",
    "Y[Y==4]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a1aeb09-1624-4f91-afe2-e6fe6ac39c0a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0aHJnLhyFK80",
    "outputId": "fe50abb7-0800-49fc-b7fa-bdc1599be315"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am in pain. My back and sides hurt. Not to mention crying is made of fail. \n"
     ]
    }
   ],
   "source": [
    "# Visualize Dataset\n",
    "\n",
    "index = 123  # index in range [0,1599999]\n",
    "\n",
    "print(X[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb1502ca-9677-427c-8edd-d05243a9eae9",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YYbJKYT-Ggis"
   },
   "outputs": [],
   "source": [
    "# Define Preprocessing functions\n",
    "\n",
    "def tokenize(X):\n",
    "  \"\"\"\n",
    "  Tokenize the data using nltk\n",
    "  \"\"\"\n",
    "\n",
    "  treebank = nltk.tokenize.TreebankWordTokenizer()\n",
    "  X_tokenized=[treebank.tokenize(sentence) for sentence in X]  \n",
    "  return X_tokenized\n",
    "\n",
    "\n",
    "def remove_stopwords(X):\n",
    "  \"\"\"\n",
    "  Remove Stopwords using nltk\n",
    "  \"\"\"\n",
    "\n",
    "  stopwords=nltk.corpus.stopwords.words('english') + ['@']\n",
    "  X_without_stopwords = []\n",
    "\n",
    "  for sentence in X:\n",
    "\n",
    "    temp = [word for word in sentence if not word in stopwords]\n",
    "    X_without_stopwords.append(temp) \n",
    "\n",
    "  return X_without_stopwords\n",
    "\n",
    "\n",
    "def stem(X,type='porter'):\n",
    "  \"\"\"\n",
    "  Perform Stemming using nltk\n",
    "  type = 'Porter','Snowball','Lancaster'\n",
    "  \"\"\"\n",
    "  \n",
    "  if type == 'porter':\n",
    "    stemmer= nltk.stem.PorterStemmer()\n",
    "  elif type == 'snowball':\n",
    "    stemmer = nltk.stem.SnowballStemmer()\n",
    "  elif type == 'lancaster':\n",
    "    stemmer = nltk.stem.LancasterStemmer()    \n",
    "\n",
    "  \n",
    "  X_stemmed = []\n",
    "\n",
    "  for sentence in X:\n",
    "\n",
    "    temp = [stemmer.stem(word) for word in sentence]\n",
    "    X_stemmed.append(temp)\n",
    "\n",
    "  return X_stemmed \n",
    "\n",
    "\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "        \"\"\"\n",
    "        return WORDNET POS compliance to WORDENT lemmatization (a,n,r,v) \n",
    "        \"\"\"\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return 'a'\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return 'v'\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return 'n'\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return 'r'\n",
    "        else:\n",
    "            return 'n'\n",
    "\n",
    "\n",
    "def lemmatize(X):\n",
    "  \"\"\"\n",
    "  Lemmatize words using corresponding POS tag\n",
    "  \"\"\"\n",
    "\n",
    "  lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "  \n",
    "  X_pos = []\n",
    "  X_lemmatized = []\n",
    "\n",
    "  for sentence in X :\n",
    "\n",
    "    temp = nltk.pos_tag(sentence)\n",
    "    X_pos.append(temp)  \n",
    "\n",
    "  for sentence in X_pos :\n",
    "\n",
    "    temp = [ lemmatizer.lemmatize(word[0],pos=get_wordnet_pos(word[1])) for word in sentence]\n",
    "    X_lemmatized.append(temp)  \n",
    "\n",
    "  return X_lemmatized    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6d20a0-3963-4d33-9365-a9dd59f7ae21",
   "metadata": {
    "colab_type": "text",
    "id": "hgesb5eM9dz7"
   },
   "source": [
    "# **Training on Pre-Processed data with GloVe Word Embeddings**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4704777f-b34b-4b49-bfca-0d8c4b60a4d1",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vGnvHQFI9jYR"
   },
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "\n",
    "X_tokenized = tokenize (X)\n",
    "\n",
    "X_without_stopwords = remove_stopwords ( X_tokenized )\n",
    "\n",
    "X_lemmatized = lemmatize ( X_without_stopwords )\n",
    "\n",
    "X_clean = []\n",
    "\n",
    "for sentence in X_lemmatized:\n",
    "\n",
    "  temp = \" \".join(sentence)\n",
    "  X_clean.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28386e1e-131a-4e1b-a5ac-f3daf7e87581",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "099aSlCgdSuH",
    "outputId": "f41677a6-ff10-4b6f-cde7-fef004193127"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of distinct tokens = 836890\n"
     ]
    }
   ],
   "source": [
    "# Count total no. of distinct tokens\n",
    "\n",
    "tokenizer = Tokenizer(filters='@')\n",
    "tokenizer.fit_on_texts(X_clean)\n",
    "\n",
    "print('No. of distinct tokens = '+str(len(tokenizer.word_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86e4f7f7-b82b-4850-b68d-f6412f822e10",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KGZbksYQdSuN"
   },
   "outputs": [],
   "source": [
    "# Define Vocabulary size (no. of most frequent tokens) to consider\n",
    "\n",
    "max_vocab=50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "187a1606-602a-457d-8beb-e9f88da85553",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eUZATHUKdSuQ"
   },
   "outputs": [],
   "source": [
    "# Reload Twitter dataset with new Vocabulary\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_vocab,filters='@')\n",
    "tokenizer.fit_on_texts(X_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4a1362b-2445-49db-835a-be83abe2f45f",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vRu7fr1T_b3Y"
   },
   "outputs": [],
   "source": [
    "# Vectorize input text using Vocabulary\n",
    "\n",
    "X_clean_vectorized=tokenizer.texts_to_sequences(X_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13117576-5e75-4d0c-88e2-ebc1eb0b431c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "u7XIoIzPdSuU",
    "outputId": "08402107-f293-4e39-83fa-c68d38edf0cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of tweets = 10.061133125\n"
     ]
    }
   ],
   "source": [
    "# Count average length of tweets\n",
    "\n",
    "length=[]\n",
    "for sentence in X_clean_vectorized:\n",
    "  length.append(len(sentence))\n",
    "  \n",
    "print('Average length of tweets = '+str(np.mean(length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f9e6e8b-55c5-4f10-8e00-3b642f7ad3f0",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pDhzyLd5dSuX"
   },
   "outputs": [],
   "source": [
    "# Define Maximum input length of the Model\n",
    "\n",
    "max_length=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3551e0b2-46f4-4061-90ca-6e7db90040e1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xGFaD8wW_b3b",
    "outputId": "47e59f0b-de0a-46ef-8dd4-8f77d32493dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600000, 20)\n"
     ]
    }
   ],
   "source": [
    "# Pad or Trim data to defined input length\n",
    "\n",
    "X_clean_pad = keras.preprocessing.sequence.pad_sequences(X_clean_vectorized,max_length,padding='post',\n",
    "                                                         truncating='post')\n",
    "\n",
    "print(X_clean_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76534ff3-7724-4d4e-aaf3-52c9957114d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "colab_type": "code",
    "id": "h7oQjX--G3IY",
    "outputId": "fe6979ae-a6c9-4335-f827-b5c027b4006f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original :\n",
      "I am in pain. My back and sides hurt. Not to mention crying is made of fail. \n",
      "\n",
      "Tokenized :\n",
      "['I', 'am', 'in', 'pain.', 'My', 'back', 'and', 'sides', 'hurt.', 'Not', 'to', 'mention', 'crying', 'is', 'made', 'of', 'fail', '.']\n",
      "\n",
      "Stopwords removed :\n",
      "['I', 'pain.', 'My', 'back', 'sides', 'hurt.', 'Not', 'mention', 'crying', 'made', 'fail', '.']\n",
      "\n",
      "POS tagged :\n",
      "[('I', 'PRP'), ('pain.', 'VBP'), ('My', 'PRP$'), ('back', 'NN'), ('sides', 'NNS'), ('hurt.', 'VBP'), ('Not', 'RB'), ('mention', 'NN'), ('crying', 'VBG'), ('made', 'VBN'), ('fail', 'NN'), ('.', '.')]\n",
      "\n",
      "Lemmatized :\n",
      "['I', 'pain.', 'My', 'back', 'side', 'hurt.', 'Not', 'mention', 'cry', 'make', 'fail', '.']\n",
      "\n",
      "Clean :\n",
      "I pain. My back side hurt. Not mention cry make fail .\n",
      "\n",
      "Vectorized :\n",
      "[2, 3428, 62, 30, 591, 4229, 146, 831, 308, 33, 426, 4]\n",
      "\n",
      "Padded :\n",
      "[   2 3428   62   30  591 4229  146  831  308   33  426    4    0    0\n",
      "    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# Visualize pre-processed data\n",
    "\n",
    "index  = 123  # index in range [0,1599999]\n",
    "\n",
    "print('\\nOriginal :')\n",
    "print(X[index])\n",
    "print('\\nTokenized :')\n",
    "print(X_tokenized[index])\n",
    "print('\\nStopwords removed :')\n",
    "print(X_without_stopwords[index])\n",
    "print('\\nPOS tagged :')\n",
    "print(nltk.pos_tag(X_without_stopwords[index]))\n",
    "print('\\nLemmatized :')\n",
    "print(X_lemmatized[index])\n",
    "print('\\nClean :')\n",
    "print(X_clean[index])\n",
    "print('\\nVectorized :')\n",
    "print(X_clean_vectorized[index])\n",
    "print('\\nPadded :')\n",
    "print(X_clean_pad[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29b76b11-55e7-4954-9439-89f143687d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split (X_clean_pad,Y.reshape(Y.shape[0],1),test_size=0.1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1d41b09-146a-4620-b1c6-3d6a83387671",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/x_train', x_train)\n",
    "np.save('data/y_train', y_train)\n",
    "np.save('data/x_val', x_val)\n",
    "np.save('data/y_val', y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433303a9-a448-4eb5-84c6-cdbb612001bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
